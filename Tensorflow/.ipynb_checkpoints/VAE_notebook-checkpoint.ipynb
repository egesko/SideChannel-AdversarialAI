{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Sampling layer as a subclass of keras.layers.Layer\n",
    "## Sampling layer: Layer that samples a random point in latent space from a distribution with a mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define latent space dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "TBD"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 6000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 3000, 256)    4352        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 1500, 128)    262272      ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 750, 32)      32800       ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 24000)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           384016      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 683,508\n",
      "Trainable params: 683,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 19:48:58.720319: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-15 19:48:59.309901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22306 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = keras.Input(shape=(6000,1))\n",
    "\n",
    "x = layers.Conv1D(256,16,strides=2,padding='same',activation='relu',use_bias=True,kernel_initializer='VarianceScaling',bias_initializer = 'Zeros')(encoder_inputs)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "x = layers.Conv1D(128,8,strides=2,padding='same',activation='relu',use_bias=True,kernel_initializer='VarianceScaling',bias_initializer = 'Zeros')(x)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "x = layers.Conv1D(32,8,strides=2,padding='same',activation='relu',use_bias=True,kernel_initializer='VarianceScaling',bias_initializer = 'Zeros')(x)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "flatten_1 = layers.Flatten()(x)\n",
    "x = layers.Dense(16 , activation=\"relu\")(flatten_1)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\",kernel_initializer='Zeros',bias_initializer = 'Zeros')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\",kernel_initializer='Zeros',bias_initializer = 'Zeros')(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "TBD"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24000)             72000     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 750, 32)           0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 1500, 32)         8224      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 3000, 128)        32896     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_2 (Conv1DT  (None, 6000, 256)        524544    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_3 (Conv1DT  (None, 6000, 1)          4097      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 641,761\n",
      "Trainable params: 641,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#DECODER\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# x = layers.Dense(1500, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "#x = layers.Dense(1500, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]), activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "x = layers.Conv1DTranspose(32, 8, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv1DTranspose(128, 8, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv1DTranspose(256, 16, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "\n",
    "decoder_outputs = layers.Conv1DTranspose(1, 16, padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining subclass VAE\n",
    "## VAE is a subclass of keras.Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "TBD"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction),axis=(1)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=0))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            #total_loss = reconstruction_loss #ABSOLUTELY CHANGE!\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "## And convert it to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = pd.read_csv('/home/ege/Repo/SideChannel-AdversarialAI/Tensorflow/DataSet/trainX13.csv', header=None)\n",
    "train_Y = pd.read_csv('/home/ege/Repo/SideChannel-AdversarialAI/Tensorflow/DataSet/trainY13.csv', header=None)\n",
    "\n",
    "trainY = train_Y.to_numpy()\n",
    "trainX = train_X.to_numpy()\n",
    "trainX = np.expand_dims(trainX,axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = np.amin(trainX)\n",
    "maximum = np.amax(trainX)\n",
    "\n",
    "trainX = (trainX-minimum)/(maximum-minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": [
     "TBD"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 [==============================] - 3s 33ms/step - loss: 2216.9816 - reconstruction_loss: 2200.0488 - kl_loss: 0.0040\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2193.4949 - reconstruction_loss: 2198.4072 - kl_loss: 7.1405e-04\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2169.7324 - reconstruction_loss: 2190.6934 - kl_loss: 1.6812e-04\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2178.6730 - reconstruction_loss: 2191.8064 - kl_loss: 1.4971e-04\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2263.1919 - reconstruction_loss: 2191.9373 - kl_loss: 3.5633e-04\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2197.6132 - reconstruction_loss: 2193.1692 - kl_loss: 1.8481e-04\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2185.0271 - reconstruction_loss: 2190.3062 - kl_loss: 8.3532e-05\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2215.7878 - reconstruction_loss: 2193.6931 - kl_loss: 1.2026e-04\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2165.7357 - reconstruction_loss: 2194.8254 - kl_loss: 1.5387e-04\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2186.9964 - reconstruction_loss: 2194.5771 - kl_loss: 1.6902e-04\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2165.1578 - reconstruction_loss: 2190.0642 - kl_loss: 1.2330e-04\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2192.7694 - reconstruction_loss: 2196.3499 - kl_loss: 1.0671e-04\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2189.9266 - reconstruction_loss: 2194.7695 - kl_loss: 6.8826e-05\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2216.5479 - reconstruction_loss: 2189.4646 - kl_loss: 1.8477e-04\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2184.8505 - reconstruction_loss: 2194.1816 - kl_loss: 2.1201e-04\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2189.3573 - reconstruction_loss: 2191.4731 - kl_loss: 1.7347e-04\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2193.2512 - reconstruction_loss: 2190.0818 - kl_loss: 8.5475e-05\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2187.0220 - reconstruction_loss: 2196.6973 - kl_loss: 1.5777e-04\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2178.5671 - reconstruction_loss: 2193.5459 - kl_loss: 1.6311e-04\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2227.7341 - reconstruction_loss: 2191.8201 - kl_loss: 6.7036e-05\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2211.4243 - reconstruction_loss: 2192.2620 - kl_loss: 7.8746e-05\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2153.2049 - reconstruction_loss: 2196.0701 - kl_loss: 4.1085e-04\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2166.0646 - reconstruction_loss: 2195.1067 - kl_loss: 1.9825e-04\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2213.9535 - reconstruction_loss: 2192.3413 - kl_loss: 1.5643e-04\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2158.4052 - reconstruction_loss: 2193.3904 - kl_loss: 1.8274e-04\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2168.1780 - reconstruction_loss: 2197.9153 - kl_loss: 3.0221e-04\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2187.7634 - reconstruction_loss: 2192.8904 - kl_loss: 8.5556e-05\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2189.2164 - reconstruction_loss: 2196.3284 - kl_loss: 1.0351e-04\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2224.7840 - reconstruction_loss: 2187.4888 - kl_loss: 7.7212e-05\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2165.1020 - reconstruction_loss: 2194.2917 - kl_loss: 8.8147e-05\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2164.6088 - reconstruction_loss: 2188.5835 - kl_loss: 9.8008e-05\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2211.6289 - reconstruction_loss: 2191.6841 - kl_loss: 8.4468e-05\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2189.0564 - reconstruction_loss: 2196.8030 - kl_loss: 8.8440e-05\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2214.5807 - reconstruction_loss: 2187.6074 - kl_loss: 7.8872e-05\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2145.2341 - reconstruction_loss: 2186.9788 - kl_loss: 1.5000e-04\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2205.7512 - reconstruction_loss: 2190.0168 - kl_loss: 1.3678e-04\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2184.9987 - reconstruction_loss: 2190.9675 - kl_loss: 1.7017e-04\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2225.7626 - reconstruction_loss: 2192.4470 - kl_loss: 6.1557e-05\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2194.6057 - reconstruction_loss: 2190.2808 - kl_loss: 7.9983e-05\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2190.8054 - reconstruction_loss: 2190.2656 - kl_loss: 1.3513e-04\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2234.0482 - reconstruction_loss: 2195.6206 - kl_loss: 8.8966e-05\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2202.4312 - reconstruction_loss: 2190.0254 - kl_loss: 6.6946e-05\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2213.7045 - reconstruction_loss: 2190.8901 - kl_loss: 7.8777e-05\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2224.0938 - reconstruction_loss: 2193.4607 - kl_loss: 8.2380e-05\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2171.3556 - reconstruction_loss: 2190.4451 - kl_loss: 6.0648e-05\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2179.5217 - reconstruction_loss: 2195.8792 - kl_loss: 1.4593e-04\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2191.3729 - reconstruction_loss: 2191.9504 - kl_loss: 1.5365e-04\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2148.7579 - reconstruction_loss: 2191.2551 - kl_loss: 2.1163e-04\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2165.6014 - reconstruction_loss: 2194.8240 - kl_loss: 3.2492e-04\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 2198.3571 - reconstruction_loss: 2188.0046 - kl_loss: 4.0302e-05\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "history = vae.fit(trainX, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tf.keras.models.load_model('TrainedModel/trainedModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test reconstructed dataset with the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 13.2401 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.5183 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 10.9348 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.1222 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1589 - accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.3682 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 9.6633 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.7791 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.8139 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.2041 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.9279 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.7254 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.8160 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 11.5878 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(14):\n",
    "    \n",
    "\n",
    "    trainXCUT = trainX[i::14]\n",
    "    trainYCUT = trainY[i::14]\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder.predict(trainXCUT)\n",
    "    reconstructed_x = vae.decoder.predict(z)*(maximum-minimum)+minimum\n",
    "\n",
    "    # if(i == 5):\n",
    "    #     sampleToPredict = 0\n",
    "    #     (reconstructed_x[sampleToPredict]).tofile('prediction.csv', sep = ',')\n",
    "    #     ((trainXCUT[sampleToPredict].T)*(maximum-minimum)+minimum).tofile('predictionTARGET.csv', sep = ',')\n",
    "\n",
    "    result = classification_model.evaluate((reconstructed_x),trainYCUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[7.4486394]\n",
      "  [5.932315 ]\n",
      "  [5.7359467]\n",
      "  ...\n",
      "  [3.8509004]\n",
      "  [3.786337 ]\n",
      "  [3.6813483]]\n",
      "\n",
      " [[7.1111174]\n",
      "  [5.7287173]\n",
      "  [5.564599 ]\n",
      "  ...\n",
      "  [3.7635245]\n",
      "  [3.6840127]\n",
      "  [3.602539 ]]\n",
      "\n",
      " [[7.2215276]\n",
      "  [5.8008356]\n",
      "  [5.595108 ]\n",
      "  ...\n",
      "  [3.7892792]\n",
      "  [3.6995034]\n",
      "  [3.6251626]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7.179163 ]\n",
      "  [5.77273  ]\n",
      "  [5.6082335]\n",
      "  ...\n",
      "  [3.7901685]\n",
      "  [3.7056594]\n",
      "  [3.6240547]]\n",
      "\n",
      " [[7.347051 ]\n",
      "  [5.877773 ]\n",
      "  [5.708166 ]\n",
      "  ...\n",
      "  [3.805622 ]\n",
      "  [3.729754 ]\n",
      "  [3.6409411]]\n",
      "\n",
      " [[7.292639 ]\n",
      "  [5.842793 ]\n",
      "  [5.6771965]\n",
      "  ...\n",
      "  [3.7773445]\n",
      "  [3.7090282]\n",
      "  [3.6171415]]]\n"
     ]
    }
   ],
   "source": [
    "print(reconstructed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
