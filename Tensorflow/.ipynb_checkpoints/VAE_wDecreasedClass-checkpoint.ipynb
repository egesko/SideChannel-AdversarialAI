{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 13:33:34.693882: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-22 13:33:35.273423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22306 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "classification_model = tf.keras.models.load_model('TrainedModel/trainedModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "### And convert it to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('/home/ege/Repo/SideChannel-AdversarialAI/Tensorflow/DataSet/trainX13.csv', header=None)\n",
    "train_Y = pd.read_csv('/home/ege/Repo/SideChannel-AdversarialAI/Tensorflow/DataSet/trainY13.csv', header=None)\n",
    "\n",
    "trainY = train_Y.to_numpy()\n",
    "trainX = train_X.to_numpy()\n",
    "trainX = np.expand_dims(trainX,axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 13:33:39.013364: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    }
   ],
   "source": [
    "minimum = np.amin(trainX)\n",
    "maximum = np.amax(trainX)\n",
    "\n",
    "trainX_normalized = (trainX-minimum)/(maximum-minimum)\n",
    "\n",
    "trainX_tensor =  tf.convert_to_tensor(trainX_normalized)\n",
    "output = tf.nn.max_pool1d(trainX_tensor, 2, 2, padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Sampling layer as a subclass of keras.layers.Layer\n",
    "## Sampling layer: Layer that samples a random point in latent space from a distribution with a mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define latent space dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "TBD"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 1500, 256)    4352        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 750, 128)     524416      ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 250, 64)      65600       ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 125, 32)      16416       ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4000)         0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           64016       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 674,868\n",
      "Trainable params: 674,868\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = keras.Input(shape=(3000,1))\n",
    "\n",
    "x = layers.Conv1D(256,16,strides=2,padding='same',activation='relu')(encoder_inputs)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "x = layers.Conv1D(128,16,strides=2,padding='same',activation='relu')(x)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "x = layers.Conv1D(64,8,strides=3,padding='same',activation='relu')(x)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "x = layers.Conv1D(32,8,strides=2,padding='same',activation='relu')(x)#possibly update kernel_initializer\n",
    "#x = layers.MaxPooling1D(pool_size = 4,strides = 4, padding = 'same')(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "flatten_1 = layers.Flatten()(x)\n",
    "\n",
    "#x = layers.LSTM(32,activation='tanh',recurrent_activation='hard_sigmoid',use_bias=True,kernel_initializer='VarianceScaling',recurrent_initializer = 'orthogonal',bias_initializer='Zeros', return_sequences = True)(flatten_1) #Variance Scaling\n",
    "\n",
    "\n",
    "#x = layers.Dense(64 , activation=\"relu\")(flatten_1)\n",
    "#x = layers.Dense(32 , activation=\"relu\")(x)\n",
    "x = layers.Dense(16 , activation=\"relu\")(flatten_1)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\",kernel_initializer='Zeros',bias_initializer = 'Zeros')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\",kernel_initializer='Zeros',bias_initializer = 'Zeros')(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "TBD"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4000)              68000     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 125, 32)           0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 250, 32)          8224      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 750, 64)          16448     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_2 (Conv1DT  (None, 1500, 128)        131200    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_3 (Conv1DT  (None, 3000, 256)        524544    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_4 (Conv1DT  (None, 3000, 1)          4097      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 752,561\n",
      "Trainable params: 752,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#DECODER\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "\n",
    "x = layers.Dense(16 , activation=\"relu\")(latent_inputs)\n",
    "#x = layers.Dense(32 , activation=\"relu\")(x)\n",
    "#x = layers.Dense(64 , activation=\"relu\")(x)\n",
    "\n",
    "#x = layers.LSTM(32,activation='tanh',recurrent_activation='hard_sigmoid',use_bias=True,kernel_initializer='VarianceScaling',recurrent_initializer = 'orthogonal',bias_initializer='Zeros', return_sequences = True)(x) #Variance Scaling\n",
    "\n",
    "\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]), activation=\"relu\")(x)\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "x = layers.Conv1DTranspose(32, 8, activation=\"relu\", strides=2,padding='same')(x)\n",
    "x = layers.Conv1DTranspose(64, 8, activation=\"relu\", strides=3,padding='same')(x)\n",
    "x = layers.Conv1DTranspose(128, 16, activation=\"relu\", strides=2,padding='same')(x)\n",
    "x = layers.Conv1DTranspose(256, 16, activation=\"relu\", strides=2,padding='same')(x)\n",
    "\n",
    "\n",
    "decoder_outputs = layers.Conv1DTranspose(1, 16, padding=\"same\",activation=\"sigmoid\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Defining subclass VAE\n",
    "## VAE is a subclass of keras.Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "TBD"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction),axis=(1)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=0))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            #total_loss = reconstruction_loss #ABSOLUTELY CHANGE!\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_dataset = output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 3000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "A_Class = 0\n",
    "B_Class = 1\n",
    "    \n",
    "trainA = pooled_dataset[A_Class::14]\n",
    "trainB = pooled_dataset[B_Class::14]\n",
    "\n",
    "trainA_Out = trainY[A_Class::14]\n",
    "trainB_Out = trainY[B_Class::14]\n",
    "\n",
    "finalX = np.append(trainA, trainB,axis=0)\n",
    "finalY = np.append(trainA_Out,trainB_Out)\n",
    "\n",
    "#finalX.shape\n",
    "finalX.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": [
     "TBD"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 13:33:42.588978: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 39ms/step - loss: 1906.8945 - reconstruction_loss: 1683.4578 - kl_loss: 3.7526e-04\n",
      "Epoch 2/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1266.9723 - reconstruction_loss: 1199.9673 - kl_loss: 4.0318e-04\n",
      "Epoch 3/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1125.5100 - reconstruction_loss: 1111.1954 - kl_loss: 1.6258e-04\n",
      "Epoch 4/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1075.6127 - reconstruction_loss: 1078.8219 - kl_loss: 3.0372e-04\n",
      "Epoch 5/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1085.0570 - reconstruction_loss: 1061.4169 - kl_loss: 4.2155e-04\n",
      "Epoch 6/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1068.1388 - reconstruction_loss: 1072.5665 - kl_loss: 4.8931e-04\n",
      "Epoch 7/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1081.8763 - reconstruction_loss: 1063.7104 - kl_loss: 5.1672e-04\n",
      "Epoch 8/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1071.4470 - reconstruction_loss: 1063.7158 - kl_loss: 5.7384e-04\n",
      "Epoch 9/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1067.0112 - reconstruction_loss: 1066.0978 - kl_loss: 7.3900e-04\n",
      "Epoch 10/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1067.2204 - reconstruction_loss: 1060.7330 - kl_loss: 0.0016\n",
      "Epoch 11/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1075.3700 - reconstruction_loss: 1058.4558 - kl_loss: 0.0316\n",
      "Epoch 12/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1045.6043 - reconstruction_loss: 1058.8062 - kl_loss: 0.9062\n",
      "Epoch 13/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1067.3341 - reconstruction_loss: 1057.5193 - kl_loss: 0.8907\n",
      "Epoch 14/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1060.5447 - reconstruction_loss: 1066.5426 - kl_loss: 2.1632\n",
      "Epoch 15/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1086.5864 - reconstruction_loss: 1063.8124 - kl_loss: 1.8349\n",
      "Epoch 16/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1020.4171 - reconstruction_loss: 1056.3417 - kl_loss: 4.4138\n",
      "Epoch 17/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1051.9215 - reconstruction_loss: 1050.7432 - kl_loss: 2.6392\n",
      "Epoch 18/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1065.6589 - reconstruction_loss: 1049.5154 - kl_loss: 2.2177\n",
      "Epoch 19/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1060.2600 - reconstruction_loss: 1041.4584 - kl_loss: 5.6158\n",
      "Epoch 20/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1048.5360 - reconstruction_loss: 1043.9551 - kl_loss: 4.8985\n",
      "Epoch 21/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1037.3919 - reconstruction_loss: 1038.9348 - kl_loss: 5.9530\n",
      "Epoch 22/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1062.2221 - reconstruction_loss: 1048.6127 - kl_loss: 5.4964\n",
      "Epoch 23/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1072.6999 - reconstruction_loss: 1050.2469 - kl_loss: 7.4694\n",
      "Epoch 24/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1066.7406 - reconstruction_loss: 1047.4706 - kl_loss: 5.3533\n",
      "Epoch 25/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1057.0170 - reconstruction_loss: 1049.9495 - kl_loss: 4.6720\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "history = vae.fit(finalX, epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1\n",
      "fitting...\n",
      "Correct Amount: 44\n",
      "------------\n",
      "0, 2\n",
      "fitting...\n",
      "Correct Amount: 98\n",
      "------------\n",
      "0, 3\n",
      "fitting...\n",
      "Correct Amount: 99\n",
      "------------\n",
      "0, 4\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "0, 5\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "0, 6\n",
      "fitting...\n",
      "Correct Amount: 93\n",
      "------------\n",
      "0, 7\n",
      "fitting...\n",
      "Correct Amount: 97\n",
      "------------\n",
      "0, 8\n",
      "fitting...\n",
      "Correct Amount: 47\n",
      "------------\n",
      "0, 9\n",
      "fitting...\n",
      "Correct Amount: 40\n",
      "------------\n",
      "0, 10\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "0, 11\n",
      "fitting...\n",
      "Correct Amount: 31\n",
      "------------\n",
      "0, 12\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "0, 13\n",
      "fitting...\n",
      "Correct Amount: 99\n",
      "------------\n",
      "1, 0\n",
      "fitting...\n",
      "Correct Amount: 21\n",
      "------------\n",
      "1, 2\n",
      "fitting...\n",
      "Correct Amount: 0\n",
      "------------\n",
      "1, 3\n",
      "fitting...\n",
      "Correct Amount: 9\n",
      "------------\n",
      "1, 4\n",
      "fitting...\n",
      "Correct Amount: 9\n",
      "------------\n",
      "1, 5\n",
      "fitting...\n",
      "Correct Amount: 99\n",
      "------------\n",
      "1, 6\n",
      "fitting...\n",
      "Correct Amount: 94\n",
      "------------\n",
      "1, 7\n",
      "fitting...\n",
      "Correct Amount: 3\n",
      "------------\n",
      "1, 8\n",
      "fitting...\n",
      "Correct Amount: 59\n",
      "------------\n",
      "1, 9\n",
      "fitting...\n",
      "Correct Amount: 43\n",
      "------------\n",
      "1, 10\n",
      "fitting...\n",
      "Correct Amount: 0\n",
      "------------\n",
      "1, 11\n",
      "fitting...\n",
      "Correct Amount: 97\n",
      "------------\n",
      "1, 12\n",
      "fitting...\n",
      "Correct Amount: 98\n",
      "------------\n",
      "1, 13\n",
      "fitting...\n",
      "Correct Amount: 98\n",
      "------------\n",
      "2, 0\n",
      "fitting...\n",
      "Correct Amount: 99\n",
      "------------\n",
      "2, 1\n",
      "fitting...\n",
      "Correct Amount: 2\n",
      "------------\n",
      "2, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "2, 4\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "2, 5\n",
      "fitting...\n",
      "Correct Amount: 46\n",
      "------------\n",
      "2, 6\n",
      "fitting...\n",
      "Correct Amount: 51\n",
      "------------\n",
      "2, 7\n",
      "fitting...\n",
      "Correct Amount: 0\n",
      "------------\n",
      "2, 8\n",
      "fitting...\n",
      "Correct Amount: 44\n",
      "------------\n",
      "2, 9\n",
      "fitting...\n",
      "Correct Amount: 43\n",
      "------------\n",
      "2, 10\n",
      "fitting...\n",
      "Correct Amount: 30\n",
      "------------\n",
      "2, 11\n",
      "fitting...\n",
      "Correct Amount: 49\n",
      "------------\n",
      "2, 12\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "2, 13\n",
      "fitting...\n",
      "Correct Amount: 48\n",
      "------------\n",
      "3, 0\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 1\n",
      "fitting...\n",
      "Correct Amount: 56\n",
      "------------\n",
      "3, 2\n",
      "fitting...\n",
      "Correct Amount: 98\n",
      "------------\n",
      "3, 4\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 5\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 6\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 7\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 8\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 9\n",
      "fitting...\n",
      "Correct Amount: 91\n",
      "------------\n",
      "3, 10\n",
      "fitting...\n",
      "Correct Amount: 90\n",
      "------------\n",
      "3, 11\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 12\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "3, 13\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "4, 0\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "4, 1\n",
      "fitting...\n",
      "Correct Amount: 59\n",
      "------------\n",
      "4, 2\n",
      "fitting...\n",
      "Correct Amount: 64\n",
      "------------\n",
      "4, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "4, 5\n",
      "fitting...\n",
      "Correct Amount: 86\n",
      "------------\n",
      "4, 6\n",
      "fitting...\n",
      "Correct Amount: 93\n",
      "------------\n",
      "4, 7\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "4, 8\n",
      "fitting...\n",
      "Correct Amount: 82\n",
      "------------\n",
      "4, 9\n",
      "fitting...\n",
      "Correct Amount: 73\n",
      "------------\n",
      "4, 10\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "4, 11\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "4, 12\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "4, 13\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "5, 0\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "5, 1\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "5, 2\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "5, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "5, 4\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "5, 6\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "5, 7\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "5, 8\n",
      "fitting...\n",
      "Correct Amount: 64\n",
      "------------\n",
      "5, 9\n",
      "fitting...\n",
      "Correct Amount: 57\n",
      "------------\n",
      "5, 10\n",
      "fitting...\n",
      "Correct Amount: 51\n",
      "------------\n",
      "5, 11\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "5, 12\n",
      "fitting...\n",
      "Correct Amount: 65\n",
      "------------\n",
      "5, 13\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "6, 0\n",
      "fitting...\n",
      "Correct Amount: 78\n",
      "------------\n",
      "6, 1\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "6, 2\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "6, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "6, 4\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "6, 5\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "6, 7\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "6, 8\n",
      "fitting...\n",
      "Correct Amount: 67\n",
      "------------\n",
      "6, 9\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "6, 10\n",
      "fitting...\n",
      "Correct Amount: 56\n",
      "------------\n",
      "6, 11\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "6, 12\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "6, 13\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "7, 0\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "7, 1\n",
      "fitting...\n",
      "Correct Amount: 1\n",
      "------------\n",
      "7, 2\n",
      "fitting...\n",
      "Correct Amount: 0\n",
      "------------\n",
      "7, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "7, 4\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "7, 5\n",
      "fitting...\n",
      "Correct Amount: 45\n",
      "------------\n",
      "7, 6\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "7, 8\n",
      "fitting...\n",
      "Correct Amount: 49\n",
      "------------\n",
      "7, 9\n",
      "fitting...\n",
      "Correct Amount: 20\n",
      "------------\n",
      "7, 10\n",
      "fitting...\n",
      "Correct Amount: 40\n",
      "------------\n",
      "7, 11\n",
      "fitting...\n",
      "Correct Amount: 37\n",
      "------------\n",
      "7, 12\n",
      "fitting...\n",
      "Correct Amount: 47\n",
      "------------\n",
      "7, 13\n",
      "fitting...\n",
      "Correct Amount: 38\n",
      "------------\n",
      "8, 0\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "8, 1\n",
      "fitting...\n",
      "Correct Amount: 52\n",
      "------------\n",
      "8, 2\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "8, 3\n",
      "fitting...\n",
      "Correct Amount: 97\n",
      "------------\n",
      "8, 4\n",
      "fitting...\n",
      "Correct Amount: 84\n",
      "------------\n",
      "8, 5\n",
      "fitting...\n",
      "Correct Amount: 61\n",
      "------------\n",
      "8, 6\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "8, 7\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "8, 9\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "8, 10\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "8, 11\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "8, 12\n",
      "fitting...\n",
      "Correct Amount: 75\n",
      "------------\n",
      "8, 13\n",
      "fitting...\n",
      "Correct Amount: 73\n",
      "------------\n",
      "9, 0\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "9, 1\n",
      "fitting...\n",
      "Correct Amount: 46\n",
      "------------\n",
      "9, 2\n",
      "fitting...\n",
      "Correct Amount: 30\n",
      "------------\n",
      "9, 3\n",
      "fitting...\n",
      "Correct Amount: 93\n",
      "------------\n",
      "9, 4\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "9, 5\n",
      "fitting...\n",
      "Correct Amount: 58\n",
      "------------\n",
      "9, 6\n",
      "fitting...\n",
      "Correct Amount: 54\n",
      "------------\n",
      "9, 7\n",
      "fitting...\n",
      "Correct Amount: 40\n",
      "------------\n",
      "9, 8\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "9, 10\n",
      "fitting...\n",
      "Correct Amount: 40\n",
      "------------\n",
      "9, 11\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "9, 12\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "9, 13\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "10, 0\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "10, 1\n",
      "fitting...\n",
      "Correct Amount: 0\n",
      "------------\n",
      "10, 2\n",
      "fitting...\n",
      "Correct Amount: 40\n",
      "------------\n",
      "10, 3\n",
      "fitting...\n",
      "Correct Amount: 93\n",
      "------------\n",
      "10, 4\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "10, 5\n",
      "fitting...\n",
      "Correct Amount: 48\n",
      "------------\n",
      "10, 6\n",
      "fitting...\n",
      "Correct Amount: 52\n",
      "------------\n",
      "10, 7\n",
      "fitting...\n",
      "Correct Amount: 40\n",
      "------------\n",
      "10, 8\n",
      "fitting...\n",
      "Correct Amount: 49\n",
      "------------\n",
      "10, 9\n",
      "fitting...\n",
      "Correct Amount: 25\n",
      "------------\n",
      "10, 11\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "10, 12\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "10, 13\n",
      "fitting...\n",
      "Correct Amount: 51\n",
      "------------\n",
      "11, 0\n",
      "fitting...\n",
      "Correct Amount: 63\n",
      "------------\n",
      "11, 1\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "11, 2\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "11, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "11, 4\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "11, 5\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "11, 6\n",
      "fitting...\n",
      "Correct Amount: 0\n",
      "------------\n",
      "11, 7\n",
      "fitting...\n",
      "Correct Amount: 51\n",
      "------------\n",
      "11, 8\n",
      "fitting...\n",
      "Correct Amount: 61\n",
      "------------\n",
      "11, 9\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "11, 10\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "11, 12\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "11, 13\n",
      "fitting...\n",
      "Correct Amount: 41\n",
      "------------\n",
      "12, 0\n",
      "fitting...\n",
      "Correct Amount: 98\n",
      "------------\n",
      "12, 1\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "12, 2\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "12, 3\n",
      "fitting...\n",
      "Correct Amount: 100\n",
      "------------\n",
      "12, 4\n",
      "fitting...\n",
      "Correct Amount: 80\n",
      "------------\n",
      "12, 5\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "12, 6\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "12, 7\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "12, 8\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "12, 9\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "12, 10\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "12, 11\n",
      "fitting...\n",
      "Correct Amount: 62\n",
      "------------\n",
      "12, 13\n",
      "fitting...\n",
      "Correct Amount: 80\n",
      "------------\n",
      "13, 0\n",
      "fitting...\n",
      "Correct Amount: 28\n",
      "------------\n",
      "13, 1\n",
      "fitting...\n",
      "Correct Amount: 43\n",
      "------------\n",
      "13, 2\n",
      "fitting...\n",
      "Correct Amount: 38\n",
      "------------\n",
      "13, 3\n",
      "fitting...\n",
      "Correct Amount: 98\n",
      "------------\n",
      "13, 4\n",
      "fitting...\n",
      "Correct Amount: 75\n",
      "------------\n",
      "13, 5\n",
      "fitting...\n",
      "Correct Amount: 60\n",
      "------------\n",
      "13, 6\n",
      "fitting...\n",
      "Correct Amount: 9\n",
      "------------\n",
      "13, 7\n",
      "fitting...\n",
      "Correct Amount: 50\n",
      "------------\n",
      "13, 8\n",
      "fitting...\n",
      "Correct Amount: 70\n",
      "------------\n",
      "13, 9\n",
      "fitting...\n",
      "Correct Amount: 54\n",
      "------------\n",
      "13, 10\n",
      "fitting...\n",
      "Correct Amount: 51\n",
      "------------\n",
      "13, 11\n",
      "fitting...\n",
      "Correct Amount: 33\n",
      "------------\n",
      "13, 12\n",
      "fitting...\n",
      "Correct Amount: 90\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#z_mean, z_log_var, z = vae.encoder.predict(finalX[0].T)\n",
    "#print(z)\n",
    "#reconstructed_x = vae.decoder.predict(z)\n",
    "\n",
    "#classification_model.evaluate(trainX_normalized,trainY)\n",
    "\n",
    "#results = classification_model.predict(trainX_normalized,trainY)\n",
    "\n",
    "x = np.linspace(-5, 5, num=10)\n",
    "y = np.linspace(-5, 5, num=10)\n",
    "\n",
    "resultMatrix = np.zeros(shape=(14,14))\n",
    "\n",
    "\n",
    "for classA in range(14):\n",
    "    for classB in range(14):\n",
    "        amtCorrect = 0\n",
    "        \n",
    "        if(classA != classB):\n",
    "            print(str(classA)+\", \"+str(classB))\n",
    "            \n",
    "            trainA = pooled_dataset[classA::14]\n",
    "            trainB = pooled_dataset[classB::14]\n",
    "\n",
    "            trainA_Out = trainY[classA::14]\n",
    "            trainB_Out = trainY[classB::14]\n",
    "\n",
    "            finalX = np.append(trainA, trainB,axis=0)\n",
    "            finalY = np.append(trainA_Out,trainB_Out)\n",
    "            \n",
    "            vae = VAE(encoder, decoder)\n",
    "            vae.compile(optimizer=keras.optimizers.Adam())\n",
    "            print(\"fitting...\")\n",
    "            history = vae.fit(finalX, epochs=15, batch_size=32,verbose=0)\n",
    "            \n",
    "            for x_i in x:\n",
    "                for y_i in y:\n",
    "                    z = np.array([[x_i,y_i]])\n",
    "                    reconstructed_x = vae.decoder.predict(z)\n",
    "                    prediction = np.argmax(classification_model.predict(reconstructed_x))\n",
    "                    \n",
    "                    if(prediction == classA or prediction == classB):\n",
    "                        amtCorrect = amtCorrect + 1\n",
    "            print(\"Correct Amount: \"+ str(amtCorrect))\n",
    "            resultMatrix[classA,classB] = amtCorrect\n",
    "            print(\"------------\")\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "        \n",
    "\n",
    "#np.argmax(results,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print reconstructed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classToCut = 8\n",
    "trainXCUT = trainX_normalized[classToCut::14]\n",
    "trainYCUT = trainY[classToCut::14]\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder.predict(finalX)\n",
    "reconstructed_x = vae.decoder.predict(z)\n",
    "\n",
    "#*(maximum-minimum)+minimum\n",
    "\n",
    "fig = plt.figure(figsize=(35,5))\n",
    "#plt.plot(results)\n",
    "\n",
    "sampleToPredict = 15\n",
    "\n",
    "plt.plot(reconstructed_x[sampleToPredict],label='Reconstruction')\n",
    "plt.plot(trainXCUT[sampleToPredict],label='Sample')\n",
    "#plt.plot(data3[0],label=3)\n",
    "#plt.plot(data4[0],label=4)\n",
    "#plt.plot(averageArray[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.yticks(np.arange(0, 9, 1))\n",
    "plt.xticks(np.arange(0, 6000, 500))\n",
    "plt.grid()\n",
    "#plt.axhline(linewidth=1, color='r')\n",
    "plt.xlabel(\"5 ms\")\n",
    "plt.ylabel(\"PnP timing\")\n",
    "#figure(figsize=(8, 6), dpi=80)\n",
    "fig.savefig('vis_test.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
